{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Versions:\n",
    "Python @ 3.8.9\n",
    "Scikit-Learn @ 1.0.2\n",
    "Numpy @ 1.21.5\n",
    "Matplotlib @ 3.5.1\n",
    "Tensorflow @ 2.8.0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "# import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people0_place1 = np.array(np.load(\"datasets/people0_place1.npy\"))\n",
    "people1_place1 = np.array(np.load(\"datasets/people1_place1.npy\"))\n",
    "people2_place1 = np.array(np.load(\"datasets/people2_place1.npy\"))\n",
    "people3_place1 = np.array(np.load(\"datasets/people3_place1.npy\"))\n",
    "people0_place2 = np.array(np.load(\"datasets/people0_place2.npy\"))\n",
    "people1_place2 = np.array(np.load(\"datasets/people1_place2.npy\"))\n",
    "people2_place2 = np.array(np.load(\"datasets/people2_place2.npy\"))\n",
    "people3_place2 = np.array(np.load(\"datasets/people3_place2.npy\"))\n",
    "people2_nonblocking = np.array(np.load(\"datasets/people2_nonblocking.npy\"))\n",
    "people3_nonblocking = np.array(np.load(\"datasets/people3_nonblocking.npy\"))\n",
    "\n",
    "data = np.concatenate((people0_place1,people1_place1,people2_place1,people3_place1[1000:], people3_nonblocking[1000:]))\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Min-Max scaling in order to amplify the higher values from the acquistion.\n",
    "\"\"\"\n",
    "scaler = MinMaxScaler()\n",
    "for i in range(len(data)):\n",
    "    for j in range(3):\n",
    "        data[i][j] = scaler.fit_transform(data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create labels.\n",
    "\"\"\"\n",
    "num_classes = 4\n",
    "labels = np.zeros((len(data),1))\n",
    "for i in range(num_classes-1):\n",
    "    labels[(i+1)*int((len(data)/4)):] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One hot encoding the labels.\n",
    "\"\"\"\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=4, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_doppler_map = processing.processing_rangeDopplerData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Reshaping the data, thus the dimensionality reduction on the Convolution Layer happens on the Chirp and Sample dimensions.\n",
    "\"\"\"\n",
    "data = data.reshape(len(data),64,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Splitting the data.\n",
    "\"\"\"\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = 0.20, random_state = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"x _train data shape: \",x_train.shape)\n",
    "print(\"y_train data shape: \",y_train.shape)\n",
    "print(\"x_test data shape: \",x_test.shape)\n",
    "print(\"y test data shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "320/320 [==============================] - 12s 33ms/step - loss: 1.0000 - accuracy: 0.5180 - val_loss: 0.6987 - val_accuracy: 0.6637\n",
      "Epoch 2/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.7028 - accuracy: 0.6667 - val_loss: 0.6402 - val_accuracy: 0.6712\n",
      "Epoch 3/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.6234 - accuracy: 0.7194 - val_loss: 0.5446 - val_accuracy: 0.7412\n",
      "Epoch 4/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.5169 - accuracy: 0.7616 - val_loss: 0.4366 - val_accuracy: 0.7937\n",
      "Epoch 5/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.4710 - accuracy: 0.7837 - val_loss: 0.5192 - val_accuracy: 0.7694\n",
      "Epoch 6/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.4197 - accuracy: 0.8100 - val_loss: 0.4018 - val_accuracy: 0.8262\n",
      "Epoch 7/20\n",
      "320/320 [==============================] - 10s 30ms/step - loss: 0.3591 - accuracy: 0.8427 - val_loss: 0.3502 - val_accuracy: 0.8381\n",
      "Epoch 8/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.3610 - accuracy: 0.8383 - val_loss: 0.6087 - val_accuracy: 0.7506\n",
      "Epoch 9/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.3494 - accuracy: 0.8466 - val_loss: 0.3642 - val_accuracy: 0.8181\n",
      "Epoch 10/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.3086 - accuracy: 0.8602 - val_loss: 0.3275 - val_accuracy: 0.8487\n",
      "Epoch 11/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.2895 - accuracy: 0.8744 - val_loss: 0.4335 - val_accuracy: 0.8306\n",
      "Epoch 12/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.2830 - accuracy: 0.8739 - val_loss: 0.3199 - val_accuracy: 0.8500\n",
      "Epoch 13/20\n",
      "320/320 [==============================] - 9s 29ms/step - loss: 0.2922 - accuracy: 0.8697 - val_loss: 0.2579 - val_accuracy: 0.8900\n",
      "Epoch 14/20\n",
      "320/320 [==============================] - 9s 30ms/step - loss: 0.2650 - accuracy: 0.8883 - val_loss: 0.5441 - val_accuracy: 0.8069\n",
      "Epoch 15/20\n",
      "320/320 [==============================] - 9s 30ms/step - loss: 0.2428 - accuracy: 0.8972 - val_loss: 0.2482 - val_accuracy: 0.8969\n",
      "Epoch 16/20\n",
      "320/320 [==============================] - 10s 30ms/step - loss: 0.2315 - accuracy: 0.9062 - val_loss: 0.2733 - val_accuracy: 0.8744\n",
      "Epoch 17/20\n",
      "320/320 [==============================] - 10s 30ms/step - loss: 0.2189 - accuracy: 0.9072 - val_loss: 0.3497 - val_accuracy: 0.8550\n",
      "Epoch 18/20\n",
      "320/320 [==============================] - 10s 30ms/step - loss: 0.1877 - accuracy: 0.9203 - val_loss: 0.2824 - val_accuracy: 0.8725\n",
      "Epoch 19/20\n",
      "320/320 [==============================] - 10s 30ms/step - loss: 0.2096 - accuracy: 0.9148 - val_loss: 0.4415 - val_accuracy: 0.8025\n",
      "Epoch 20/20\n",
      "320/320 [==============================] - 10s 30ms/step - loss: 0.2023 - accuracy: 0.9183 - val_loss: 0.3402 - val_accuracy: 0.8556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1600793d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(64,128,3),padding='same'))\n",
    "model.add(layers.LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='linear',padding='same'))\n",
    "model.add(layers.LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='linear',padding='same'))\n",
    "model.add(layers.LeakyReLU(alpha=0.1))                  \n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='linear'))\n",
    "model.add(layers.LeakyReLU(alpha=0.1))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(128, activation='linear'))\n",
    "model.add(layers.LeakyReLU(alpha=0.1))               \n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#fitting the data into the model\n",
    "model.fit(x_train,y_train,batch_size=20, epochs=20,validation_data=(x_test,y_test),verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[414   1   0   0]\n",
      " [  0 331  36   7]\n",
      " [  0 158 231   5]\n",
      " [  0  17   7 393]]\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       415\n",
      "           1       0.65      0.89      0.75       374\n",
      "           2       0.84      0.59      0.69       394\n",
      "           3       0.97      0.94      0.96       417\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.87      0.85      0.85      1600\n",
      "weighted avg       0.87      0.86      0.85      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "confusion_m = confusion_matrix(y_test.argmax(axis=-1), y_pred.argmax(axis=-1))\n",
    "classification_rep = classification_report(y_test.argmax(axis=-1), y_pred.argmax(axis=-1))\n",
    "\n",
    "print(\"Confusion Matrix\\n\", confusion_m)\n",
    "print(\"Classification Report\\n\",classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving the model.\n",
    "\"\"\"\n",
    "model.save(\"my_model_noblockin3_old_data.h5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a21f821499dd7df2057c26afca79f581da3d3fbf40169bd900004c4860519f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
